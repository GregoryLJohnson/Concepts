{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CNN_classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, L, num_class):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = 3, stride = 1, padding = 1, bias=False)\n",
    "        self.pool = torch.nn.Identity()\n",
    "        #self.pool = torch.nn.MaxPool2d( kernel_size = 2)\n",
    "\n",
    "        self._dim_to_linear = None\n",
    "        self._get_flattened_conv_output_size(L)\n",
    "\n",
    "        self.fc = torch.nn.Linear(self._dim_to_linear, num_class)\n",
    "\n",
    "    def _get_flattened_conv_output_size(self, L):\n",
    "        \"\"\"Pass a dummy tensor through conv layers to compute FC layer size\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Pass a dummy input with shape (1, 1, L, L) to match the Conv2d input requirements\n",
    "            # conv layers expect (batch_size, num_channels, ...data dimension... )\n",
    "            dummy_input = torch.randn(1, 1, L, L)\n",
    "            dummy_output = self.pool(self.conv(dummy_input))\n",
    "            self._dim_to_linear = dummy_output.numel()  # Compute feature count\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv(x)))\n",
    "        x = torch.flatten(x, start_dim = 1)  # Flatten from the first non-batch dimension\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "image, label = train_data[0]\n",
    "L = image.shape[1]\n",
    "num_class = len(set([label for image, label in train_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2949, Accuracy: 92.28%\n",
      "Epoch 2/10, Loss: 0.1121, Accuracy: 96.91%\n",
      "Epoch 3/10, Loss: 0.0853, Accuracy: 97.53%\n",
      "Epoch 4/10, Loss: 0.0720, Accuracy: 97.90%\n",
      "Epoch 5/10, Loss: 0.0630, Accuracy: 98.09%\n",
      "Epoch 6/10, Loss: 0.0558, Accuracy: 98.36%\n",
      "Epoch 7/10, Loss: 0.0512, Accuracy: 98.48%\n",
      "Epoch 8/10, Loss: 0.0465, Accuracy: 98.61%\n",
      "Epoch 9/10, Loss: 0.0425, Accuracy: 98.69%\n",
      "Epoch 10/10, Loss: 0.0391, Accuracy: 98.80%\n"
     ]
    }
   ],
   "source": [
    "model = CNN_classifier( L = L, num_class = num_class)\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0693, Test Accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_data = len(test_loader.dataset)\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, correct = 0.0, 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "\n",
    "        test_loss += criterion(outputs, labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100*correct/num_data\n",
    "    average_loss = test_loss/num_batches\n",
    "\n",
    "print(f\"Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier as a thermometer for the Ising model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- background packages\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "#---- parallel computation packages \n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def hotstart(L): \n",
    "    ''' hotstart(L)\n",
    "    LxL Matrix of random spin '''\n",
    "    return np.random.choice([-1,1],size=(L,L))\n",
    "\n",
    "def Metropolis_update(Latt, beta, N_burnin, N_measure, N_separation ):\n",
    "    H = 0\n",
    "    J = 1\n",
    "    L = len(Latt)\n",
    "    m = np.sum(Latt) # the magnetization\n",
    "    \n",
    "    e = 0\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            # move through lattice and compute s_i * neighbors\n",
    "            # this double counts bounds, so divide by 2 afterwards\n",
    "            e += -J*Latt[i, j] * (Latt[(i+1)%L, j] + Latt[(i-1)%L, j]\n",
    "                                  + Latt[i, (j+1)%L]+ Latt[i, (j-1)%L])\n",
    "    e = e/2 - H*m\n",
    "    \n",
    "    # Perform the burnin stage, don't log the observables traces\n",
    "    for s in range(N_burnin):\n",
    "        # choose a random spin to flip - convince youself of the dE expression, do it by hand for a small lattice\n",
    "        i, j = np.random.randint(0, L, 2)\n",
    "        de = -2 * (-J) * Latt[i, j] * (Latt[(i+1)%L, j] + Latt[(i-1)%L, j]\n",
    "                                            + Latt[i, (j+1)%L]+ Latt[i, (j-1)%L])\n",
    "        de = de + 2*H*Latt[i,j]\n",
    "\n",
    "        if np.random.rand() < np.exp(-beta*de):\n",
    "            Latt[i,j] *= -1 # flip the spin if random # < exp(-\\beta de)\n",
    "            e += de\n",
    "            m += 2*Latt[i,j]\n",
    "    \n",
    "    trace = np.zeros((N_measure,2)) # trace of energy and magnetization\n",
    "    trace_Latt = []\n",
    "    \n",
    "    # Perform the measurement stage, log observables only every N_separation steps to obtain N_measure samples\n",
    "    for s in range(N_measure):\n",
    "        for sep in range(N_separation):\n",
    "            # choose a random spin to flip - convince youself of the dE expression, do it by hand for a small lattice\n",
    "            i, j = np.random.randint(0, L, 2)\n",
    "            de = -2 * (-J) * Latt[i, j] * (Latt[(i+1)%L, j] + Latt[(i-1)%L, j]\n",
    "                                                + Latt[i, (j+1)%L]+ Latt[i, (j-1)%L])\n",
    "            de = de + 2*H*Latt[i,j]\n",
    "\n",
    "            if np.random.rand() < np.exp(-beta*de):\n",
    "                Latt[i,j] *= -1 # flip the spin if random # < exp(-\\beta de)\n",
    "                e += de\n",
    "                m += 2*Latt[i,j]\n",
    "\n",
    "        trace[s] = [m,e]\n",
    "        trace_Latt.append(Latt.copy())\n",
    "        ## If appending just Latt, the list will contain only \"Latt\" which is updated every step!!!\n",
    "    \n",
    "    return trace, trace_Latt\n",
    "\n",
    "#---- Ising Monte Carlo data generation\n",
    "#---- generates 100 roughly decorrelated spin configurations per temperature\n",
    "\n",
    "L = 8\n",
    "\n",
    "N_sites = L**2\n",
    "N_burnin = 100*N_sites\n",
    "N_measure = 10**2\n",
    "N_separation = 50*N_sites\n",
    "\n",
    "Nt = num_cores*5\n",
    "temperature_range = np.linspace(2, 2.8, Nt)\n",
    "temperature_labels = []\n",
    "\n",
    "for t in temperature_range:\n",
    "    for n in range(N_measure):\n",
    "        temperature_labels.append(t)\n",
    "temperature_labels = np.array(temperature_labels).reshape((-1,1))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "innerm_ = partial(Metropolis_update,  N_burnin = N_burnin, N_measure = N_measure, N_separation = N_separation )\n",
    "output = Parallel(n_jobs = num_cores)(delayed(innerm_)(hotstart(L), 1/t) for t in temperature_range)\n",
    "\n",
    "spin_configurations = np.array([out[1] for out in output])\n",
    "spin_configurations = spin_configurations.reshape((-1, 1, L, L))\n",
    "\n",
    "t1 = time()\n",
    "m, s = divmod(t1-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(f\"Computation time = {h} hours, {m} minutes, {s} seconds\")\n",
    "\n",
    "spin_configurations = torch.tensor(spin_configurations, dtype=torch.float32)\n",
    "temperature_labels = torch.tensor(np.digitize(temperature_labels.flatten(), temperature_range) - 1)\n",
    "\n",
    "train_dataset = TensorDataset(spin_configurations, temperature_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_thermometer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, L, num_labels):\n",
    "        super(CNN_thermometer, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = 3, stride = 1, padding = 1, bias=False)\n",
    "        #self.pool = torch.nn.MaxPool2d(2)\n",
    "        self.pool = torch.nn.Identity()\n",
    "\n",
    "        self._dim_to_linear = None\n",
    "        self._get_flattened_conv_output_size(L)\n",
    "\n",
    "        self.fc = torch.nn.Linear(self._dim_to_linear, num_labels)\n",
    "\n",
    "    def _get_flattened_conv_output_size(self, L):\n",
    "        \"\"\"Pass a dummy tensor through conv layers to compute FC layer size\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Pass a dummy input with shape (1, 1, L, L) to match the Conv2d input requirements\n",
    "            # conv layers expect (batch_size, num_channels, ...data dimension... )\n",
    "            dummy_input = torch.randn(1, 1, L, L)\n",
    "            dummy_output = self.pool(self.conv(dummy_input))\n",
    "            self._dim_to_linear = dummy_output.numel()  # Compute feature count\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv(x)))\n",
    "        x = torch.flatten(x, start_dim = 1)  # Flatten from the first non-batch dimension\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)  # Apply softmax across the feature dimension\n",
    "        #return x\n",
    "\n",
    "\n",
    "model = CNN_thermometer( L = L, num_labels = Nt)\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "num_epochs = 10**4 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights of the fully connected (FC) layer\n",
    "weights = model.fc.weight.data.numpy()\n",
    "\n",
    "# Get the number of output nodes (temperature labels) and hidden neurons (y-axis)\n",
    "num_labels = weights.shape[0]  # Number of output nodes (temperature labels)\n",
    "num_hidden_neurons = weights.shape[1]  # Number of hidden neurons in the linear layer\n",
    "\n",
    "# Plot the heatmap with output nodes on the x-axis and hidden neurons on the y-axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cax = ax.imshow(np.transpose(weights), cmap='hot', aspect='auto')\n",
    "\n",
    "# # Set axis labels\n",
    "ax.set_xlabel(\"Output Nodes (Temperature Labels)\")\n",
    "ax.set_ylabel(\"Hidden Neurons\")\n",
    "\n",
    "# Set x-axis ticks to correspond to the output nodes (temperature labels) with an increment of 10\n",
    "x_ticks = np.arange(0, num_labels, 10)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks)\n",
    "\n",
    "# Set y-axis ticks to correspond to the hidden neurons with an increment of 10\n",
    "y_ticks = np.arange(0, num_hidden_neurons, num_hidden_neurons // 10)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticks)\n",
    "\n",
    "# Add a color bar to show the intensity scale\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.title(\"Heatmap of Weights from the Fully Connected Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN_I_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
